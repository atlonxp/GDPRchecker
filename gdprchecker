#!/usr/bin/env python

import argparse
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from clint.textui import indent, colored, puts

parser = argparse.ArgumentParser(description='Try to give an idea of a website GDPR compliance')
parser.add_argument('url', nargs='+')

args = parser.parse_args()


def crawler_results(item, response, spider):
    form_found(dict(item))


def form_found(form):
    puts(colored.green("Found form at ") + form['url'])
    puts(colored.green("With the following fieldÂ :"))
    with indent(4):
        for input in form['inputs']:
            puts(input['name'])


def crawl(urls):
    settings = get_project_settings()
    settings.update({
        'LOG_ENABLED': False,
        'STATS_DUMP': False
    })

    process = CrawlerProcess(settings)
    crawler = process.create_crawler('form')

    crawler.signals.connect(crawler_results, scrapy.signals.item_scraped)

    process.crawl(crawler, urls=urls)
    process.start()


def main(args):
    crawl(args.url)


main(args)
