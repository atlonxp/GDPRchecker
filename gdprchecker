#!/usr/bin/env python
"""gdprchecker CLI tool"""

import argparse
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings
from clint.textui import indent, colored, puts


def crawler_result(item, **_):
    """Called for each crawler result"""
    form_found(dict(item))


def form_found(form):
    """Print each form found"""
    puts(colored.green("Found form at ") + form['url'])
    puts(colored.green("With the following field:"))
    with indent(4):
        for input_field in form['inputs']:
            puts(input_field['name'])


def crawl(urls, verbose):
    """Crawl a list of url"""
    settings = get_project_settings()
    settings.update({
        'LOG_ENABLED': verbose > 0,
        'STATS_DUMP': False
    })

    process = CrawlerProcess(settings)
    crawler = process.create_crawler('form')

    crawler.signals.connect(crawler_result, scrapy.signals.item_scraped)

    process.crawl(crawler, urls=urls)
    process.start()


def main():
    """gdprchecker CLI tool entry point"""
    parser = argparse.ArgumentParser(description=(
        'Try to give hints about some steps required to make a website GDPR compliant. '
        'If no check flag is passed do all checks.'
    ))
    parser.add_argument('url', nargs='+', help='One or many urls to check')
    parser.add_argument('-f', '--form', action='store_true', help='Check for forms')
    parser.add_argument('-s', '--https', action='store_true', help='Check for https')
    parser.add_argument('-v', '--verbose', action='count', default=0,
                        help='Increase the verbosity level')

    args = parser.parse_args()
    if not args.form and not args.https:
        args.form = True
        args.https = True

    if args.form:
        crawl(args.url, args.verbose)


if __name__ == "__main__":
    main()
